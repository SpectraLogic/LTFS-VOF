{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9ff49e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LTFS Versioned Object Format\n",
    "\n",
    "The LTFS Versioned Object Format (LTFS-VOF) is a format for saving object data to tapes. This document describes the format so that others may create compatible systems or tools. Python code is included which implements decoding of LTFS-VOF data and metadata. You can download and use this Jupyter notebook interactively to decode your own data.\n",
    "\n",
    "This specification overlays the Linear Tape File System (LTFS). An\n",
    "implementation of the Vail Tape Format will want to refer to the [LTFS\n",
    "specification][ltfs] or use a pre-built LTFS driver. LTFS provides a\n",
    "format for storing files on tape with a POSIX programming interface.\n",
    "The Versioned Object Format layers on top of one or more LTFS tapes to provide:\n",
    "\n",
    "[ltfs]: https://www.snia.org/tech_activities/standards/curr_standards/ltfs\n",
    "\n",
    "1. Efficient object and metadata packing. LTFS-VOF uses large files on\n",
    "   LTFS, which both minimizes the overhead of LTFS metadata and tape\n",
    "   load/unload time.\n",
    "\n",
    "2. Support for very small and very large object sizes. Very small\n",
    "   objects will be packed into large LTFS files, allowing rapid\n",
    "   transfer of many small objects to/from tape. Very large objects may\n",
    "   span tapes.\n",
    "\n",
    "3. S3-compliant object versioning. Unlike POSIX, objects in LTFS-VOF may\n",
    "   have multiple versions, including delete markers.\n",
    "\n",
    "4. S3-compliant object names and metadata. POSIX and S3 have different\n",
    "   naming restrictions and differences in the format of metadata. LTFS-VOF\n",
    "   captures object metadata in LTFS files, similar to object data.\n",
    "\n",
    "5. Modern compression, encryption, and hash codes. LTFS-VOF uses Zstandard\n",
    "   compression, which allows users a great deal of flexibility to\n",
    "   trade off speed vs. compression efficiency. AES-256 encryption is\n",
    "   used, with flexible AES key identifiers that may reference Amazon\n",
    "   Web Services KMS. Data integrity is assured with modern hash codes\n",
    "   such as XXHash.\n",
    "\n",
    "6. Support for tape-set parity. In configurations with multiple tape\n",
    "   libraries, parity packs may be stored to maximize data availability\n",
    "   in the event that a library is down or a set of tapes is damaged.\n",
    "\n",
    "In addition to LTFS, a LTFS-VOF implementation uses [MessagePack][msgpack] to encode various structures. MessagePack implementations are available for most programming languages.\n",
    "\n",
    "[msgpack]: https://msgpack.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c13c6",
   "metadata": {},
   "source": [
    "# Object System Concepts\n",
    "\n",
    "S3-compatible object stores have different concepts and terminology than file systems. This section provides an overview.\n",
    "\n",
    "![Buckets, Objects, Versions](figures/concepts1.pdf)\n",
    "\n",
    "### Buckets\n",
    "\n",
    "A *bucket* is the outermost container for objects; it is most similar\n",
    "to a file system. Buckets tend to have high-level policies that apply\n",
    "to all objects within them, for example lifecycle policies that\n",
    "control where data is placed and when tape copies are mode.\n",
    "\n",
    "### Objects and Versions\n",
    "\n",
    "Buckets contain *objects*. Each object has a name (or key) and other\n",
    "metadata associated with it, and a map of data blocks. Objects have\n",
    "one or more *versions*. The last version for a given name is called\n",
    "the *current version*. The object may be thought of as a pointer to\n",
    "the current version. To avoid confusion, this document will almost\n",
    "exclusively discuss versions instead of objects.\n",
    "\n",
    "In addition to the object data, a version has metadata about the version record itself, for example the version's ID, ETag, and creation time. The version metadata is also saved to tape so that the tape set will contain both the data and metadata for all objects.\n",
    "\n",
    "![Blocks and Packs](figures/concepts2.pdf)\n",
    "\n",
    "### Blocks\n",
    "\n",
    "A version's data is stored in one or more *blocks*. Each block is a\n",
    "slice of object data, typically 10MB in size before compression. An\n",
    "object whose length is less than the block length will simply be\n",
    "composed of one short block. A larger object will be composed of\n",
    "multiple blocks with a short block at the end. Each block is\n",
    "compressed and encrypted individually so that a S3 client performing\n",
    "range reads may be answered by decoding only the blocks containing the\n",
    "range the client is asking for.\n",
    "\n",
    "A set of blocks are stored together in *packs*. Packs are typically\n",
    "large files, where the optimal pack size is determined by the type of\n",
    "storage media. For tape, packs will be multiple gigabytes in size, and\n",
    "will contain hundreds of blocks stored end-to-end. Packs may contain\n",
    "blocks belonging to many versions.\n",
    "\n",
    "Packs will also contain metadata in the form of pack lists and version records, not just blocks. This metadata allows a tape set to be fully self-describing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa079b",
   "metadata": {},
   "source": [
    "# Tape Format Overview\n",
    "\n",
    "The LTFS Versioned Object Format is composed of several levels. At each level is\n",
    "an encoding scheme that is straightforward to implement, while\n",
    "providing high runtime efficiency. This section presents an overview\n",
    "of each level, with detailed descriptions in following sections.\n",
    "\n",
    "![Levels](figures/levels.pdf)\n",
    "\n",
    "\n",
    "### Level 0: LTFS\n",
    "\n",
    "The base (zero) level in the system is LTFS, as specified by the LTFS\n",
    "Format Specification version 2 or later. Any LTFS-compliant driver may\n",
    "be used. The LTFS-VOF should be implemented in a user-space\n",
    "process, using standard POSIX file system calls to manipulate the\n",
    "files on a LTFS tape. LTFS uses a combination of data blocks, index\n",
    "blocks, and file marks to lay out data on a tape. This is entirely\n",
    "transparent to the Vail Tape Format, however.\n",
    "\n",
    "### Level 1: Packs\n",
    "\n",
    "The first level is *packs*, which are LTFS files that store encoded\n",
    "data or metadata. Each pack stores either object data in the form of\n",
    "*blocks*, or metadata in the form of *versions*. Both are described in\n",
    "later sections.\n",
    "\n",
    "LTFS may use one or more extents to store a file, and each extent is a\n",
    "series of data blocks followed by an index block describing the\n",
    "extent. LTFS also stores the index block in a separate index partition\n",
    "which is read when a tape is mounted. Level one is also mostly\n",
    "transparent to a LTFS-VOF implementation.\n",
    "\n",
    "### Level 2: TLV\n",
    "\n",
    "The second level is an end-to-end stacking of records within packs.\n",
    "Each record is encoded with a tag-length-value (TLV) format that uses a\n",
    "fixed-size header and variable-length value. This header contains the\n",
    "minimal information required to identify the type of data stored, its\n",
    "length, and hash codes to validate both the header's integrity and the\n",
    "value's integrity. Any TLV may be read and decoded by simply reading\n",
    "its range of bytes within the pack.\n",
    "\n",
    "### Level 3: Value Encoding\n",
    "\n",
    "The third level uses a variable-size header that provides details on\n",
    "compression, encryption, and the application version used to encode\n",
    "the value. MessagePack is used to decode the header first, then the\n",
    "value contents must be decrypted and decompressed, then those\n",
    "block/version bytes are decoded.\n",
    "\n",
    "### Level 4: Data and Metadata\n",
    "\n",
    "The final level is the LTFS-VOF data and metadata objects. These are the\n",
    "version records, pack lists, and data blocks which comprise all the data\n",
    "stored in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57057254",
   "metadata": {},
   "source": [
    "Pack Files\n",
    "----------\n",
    "\n",
    "Packs are named with a [ULID][ulid] followed by the extension `.blk`\n",
    "for packs containing blocks, or `.ver` for packs containing versions.\n",
    "Blocks and versions are stored separately so that a system importing a\n",
    "set of tapes need only scan the `.ver` packs to build its database of\n",
    "metadata.\n",
    "\n",
    "[ulid]: https://github.com/oklog/ulid\n",
    "\n",
    "ULID is similar to a UUID, however it contains an embedded timestamp\n",
    "and sorts lexicographically from oldest time to newest time. Pack IDs\n",
    "should use the current time (in UTC) when the pack is first written to.\n",
    "\n",
    "Pack files should be stored in the root directory of a LTFS tape.\n",
    "\n",
    "It is important for read performance that pack files be stored as long\n",
    "runs of contiguous LTFS data extents. While LTFS supports interleaving\n",
    "of file data--which may happen if multiple files are written\n",
    "concurrently--this is not very efficient, as reading such a file will\n",
    "require LTFS to perform many seeks. Therefore it is strongly\n",
    "recommended that a LTFS-VOF writer sequentially write full packs to tape,\n",
    "one pack at a time, so that packs may be composed of large LTFS\n",
    "extents without on-tape interleaving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b0811",
   "metadata": {},
   "source": [
    "# TLV Encoding\n",
    "\n",
    "![TLV Header](figures/tlv_header.pdf)\n",
    "\n",
    "The tag-length-value (TLV) format is used to store many records into a\n",
    "pack file. Its role is to provide just enough information for an\n",
    "application to scan through a pack file, hop from one TLV to the next,\n",
    "and ensure that records are valid before attempting to decode them.\n",
    "\n",
    "TLV uses a 32 byte header and variable-length value. A TLV reader\n",
    "should read the header and validate it using these steps, as illustrated in `read_tlv_header()` below:\n",
    "\n",
    "1. The header \"magic\" is correct. The sequence should match\n",
    "   `0x89`, `T`, `L`, `V`, `0x0d`, `0x0a`, `0x1a`, `0x0a`. This\n",
    "   sequence identifies the TLV and allows early detection of certain\n",
    "   types of corruption, for example end-of-line mangling if the TLV\n",
    "   was accidentally treated as text. (The header magic is borrowed\n",
    "   from the PNG file format. Further detail on its rationale may be\n",
    "   found in the [PNG file format specification][png]).\n",
    "\n",
    "2. The version at byte 24 indicates the version of the\n",
    "   TLV format used when this TLV was created. This is currently 0. Decoding\n",
    "   should stop if an unknown version number is seen.\n",
    "\n",
    "3. At this point the header hash should be calculated and verified.\n",
    "   The hash type is stored at byte 27, and header hash value is stored at bytes 30..31.\n",
    "   Hash type 8, XXHash64, is standard. This is validated by\n",
    "   calculating a [XXHash64][xxhash], keeping only the lower 16\n",
    "   bits, and comparing those to the stored value. Decoding should stop\n",
    "   if the header hash code does not match.\n",
    "\n",
    "The following code demonstrates reading and unpacking the TLV header.\n",
    "\n",
    "[png]: http://www.libpng.org/pub/png/spec/1.2/PNG-Rationale.html#R.PNG-file-signature\n",
    "\n",
    "[xxhash]: http://cyan4973.github.io/xxHash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb46e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python modules for the code in this notebook.\n",
    "# Uncomment following two lines if you have import failures:\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install xxhash msgpack zstd cryptography\n",
    "\n",
    "# Import modules needed for sample code below\n",
    "import base64, io, msgpack, typing, zstd, unittest\n",
    "from pprint import pprint\n",
    "from ulid import ULID\n",
    "from collections import namedtuple\n",
    "from struct import unpack\n",
    "from typing import Optional\n",
    "from xxhash import xxh64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1501527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define namedtuple for TLV header fields\n",
    "TlvHeader = namedtuple('TlvHeader', 'magic dlen dhash version tag hashtype hhash')\n",
    "\n",
    "def read_tlv_header(f: typing.BinaryIO) -> TlvHeader:\n",
    "    \"\"\"\n",
    "    Read, validate, and decode one TLV header from a file-like IO, leaving\n",
    "    the IO positioned at the start of the value.\n",
    "    :param f: file-like IO to read from\n",
    "    :return: decoded TLV header tuple\n",
    "    \"\"\"\n",
    "    header_raw = f.read(32)\n",
    "\n",
    "    if len(header_raw) == 0:\n",
    "        raise EOFError\n",
    "\n",
    "    if len(header_raw) < 32:\n",
    "        raise RuntimeError(f'TLV header too short; need 32 bytes, got {len(header_raw)}')\n",
    "\n",
    "    h = TlvHeader._make(unpack(\"!8sQQB2sBxxH\", header_raw))\n",
    "\n",
    "    if h.magic != b'\\x89TLV\\r\\n\\x1a\\n':\n",
    "        raise 'invalid TLV header magic'\n",
    "\n",
    "    if h.version != 0:\n",
    "        raise f'unknown version {h.version}; can only handle TLV version 0'\n",
    "\n",
    "    if h.hashtype != 8:\n",
    "        raise f'invalid hash type {h.hashtype}; can only handle 8 (xxhash64)'\n",
    "\n",
    "    if h.hhash != (xxh64(header_raw[0:30]).intdigest() % 2 ** 16):\n",
    "        raise 'TLV header hash mismatch'\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e69c0",
   "metadata": {},
   "source": [
    "Now that the header has been unpacked and verified, its remaining fields may be considered trustworthy. The tag at bytes 25..26 is used to identify the data type of the value. The data length at bytes 8..15 is stored in network byte order (big-endian). Finally, The data buffer integrity should then be validated using the hash code stored at bytes 16..23. This uses the same hash type as in step 3 above. (The full 64 bits are used this time, instead of 16, as used for header validation.)\n",
    "\n",
    "To consume the data portion of the TLV, simply read the number of data bytes indicated in the header, and validate using the data hash code from the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1161e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header: TlvHeader(magic=b'\\x89TLV\\r\\n\\x1a\\n', dlen=14, dhash=16374443882442574646, version=0, tag=b'C!', hashtype=8, hhash=47892)\n",
      "data: b'data data data'\n"
     ]
    }
   ],
   "source": [
    "def read_tlv(f: typing.BinaryIO) -> (TlvHeader, bytes):\n",
    "    \"\"\"\n",
    "    Read a complete TLV from the file-like IO, leaving the IO positioned at the start of the next TLV.\n",
    "    :param f: file-like IO to read from\n",
    "    :return: TLV header tuple and value bytes\n",
    "    \"\"\"\n",
    "    header = read_tlv_header(f)\n",
    "    data = f.read(header.dlen)\n",
    "\n",
    "    if len(data) < header.dlen:\n",
    "        raise f'short data read: expected {header.dlen} bytes, got {len(data)}'\n",
    "\n",
    "    if header.dhash != xxh64(data).intdigest():\n",
    "        raise \"TLV data hash mismatch\"\n",
    "\n",
    "    return header, data\n",
    "\n",
    "# Small sample TLV, base64-encoded\n",
    "with io.BytesIO(base64.b64decode(\"iVRMVg0KGgoAAAAAAAAADuM9tfSfjss2AEMhCAAAuxRkYXRhIGRhdGEgZGF0YQ==\")) as f:\n",
    "    header, data = read_tlv(f)\n",
    "    print(f'header: {header}')\n",
    "    print(f'data: {data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854dc0ab",
   "metadata": {},
   "source": [
    "Multiple TLVs may be stored end-to-end within a file. The example file `3simple.tlv` has three TLVs with simple strings for values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77f6d5e-226d-49d5-b7b5-2582f30cc8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag b'bk' value b'data 1'\n",
      "tag b'bk' value b'data 2'\n",
      "tag b'bk' value b'data 3'\n"
     ]
    }
   ],
   "source": [
    "with open('sample_data/3simple.tlv', 'rb') as f:\n",
    "    for i in range(3):\n",
    "        header, data = read_tlv(f)\n",
    "        print(f'tag {header.tag} value {data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e111fa",
   "metadata": {},
   "source": [
    "# Value Encoding\n",
    "\n",
    "TLV values are encoded using a separate, second level called *value\n",
    "encoding*. This is separate from TLV because:\n",
    "\n",
    "1. TLV allows many items to be stored together and validated without decoding the values. This allows copying TLVs from one site to another, or migrating from one storage medium to another, even if the encryption keys are not accessible.\n",
    "\n",
    "2. Applications may scan through many TLVs looking for a specific one (identified by data hash) and then decode only the TLV it needs.\n",
    "\n",
    "3. TLV requires a fixed-size header by design. The value encoder uses a variable-size header because the encoding may have several stages, and the parameters of each stage will be specified in the header. For example, if the value is encrypted, the header will include crypt-specific details. If not encrypted, these fields will not be present.\n",
    "\n",
    "Value encoding provides the following features:\n",
    "\n",
    "1. Compression, by default using the Zstandard algorithm. If the data is not compressible, then compression may be skipped.\n",
    "   \n",
    "2. Encryption with AES-256.\n",
    "\n",
    "3. Data format versioning. If the application changes its saved data format, it will increment the version number for saved values. Decoding should inspect the version number and decode accordingly.\n",
    "   \n",
    "4. Flexibility for future features. Fields may be added to the header as necessary.\n",
    "\n",
    "[MessagePack][msgpack] is used to serialize both the header and the application-defined data. High-quality implementations of MessagePack are available for most programming languages. MessagePack is a fast, compact, binary encoding format.\n",
    "\n",
    "[msgpack]: https://msgpack.org/index.html\n",
    "\n",
    "![Value Encoding](figures/value_encoding.pdf)\n",
    "\n",
    "An encoded value has a manadatory first part describing the encoding and containing any structured data, called the primary part. It may also have an optional secondary part. The secondary part, if present, will be raw bytes which are usually compressed and/or encrypted in the same manner as the primary part.\n",
    "\n",
    "## Primary Part Decode\n",
    "\n",
    "The following code shows how to decode a value. In a nutshell, the process is:\n",
    "\n",
    "1. MessagePack decode the value, using the provided structure\n",
    "   definitions below. This first pass decodes the header; the\n",
    "   value's primary part remains encoded in the `e` field. In\n",
    "   following steps, the term *primary part* will refer to bytes\n",
    "   that are initially stored in `e` field and get passed through\n",
    "   the various decoding steps below.\n",
    "   \n",
    "2. If key `z` (crypt info) is provided, then the primary part\n",
    "   must be decrypted. Details on cipher setup and key identification follow\n",
    "   in later sections.\n",
    "   \n",
    "3. If key `c` (compression type) is provided, then use the appropriate\n",
    "   algorithm to decompress the primary part. Zstandard is the\n",
    "   default algorithm.\n",
    "\n",
    "4. The key `v` (version) is provided, it will indicate what version of structure is\n",
    "   stored. Currently all LTFS-VOF structures are version zero so key `v` will not be  \n",
    "   present. If breaking changes are made to the format, this number will\n",
    "   indicate which version is stored.\n",
    "   \n",
    "5. Now that the primary part's type is known (from TLV decode) and the\n",
    "   version is known (from prior step), use MessagePack to decode the\n",
    "   primary part into the appropriate data structure.\n",
    "   \n",
    "## Secondary Part Decode\n",
    "\n",
    "The secondary part has its own encoding parameters. Note that this field is a list to allow for multiple secondary parts. In its current form LTFS-VOF will only use one secondary part in a value.\n",
    "\n",
    "If key `s` is provided for secondary encoding parameters, the mandatory sub-key `l` will specify the encoded length of the secondary part. For all other secondary encoding parameters, they should be assumed to match the encoding of the primary part unless explicitly overridden in the secondary encoding structure.\n",
    "\n",
    "Whereas the primary part is always encoded with MessagePack, the secondary part will be raw bytes. In LTFS-VOF only data blocks will have a secondary part.\n",
    "\n",
    "## Encryption\n",
    "\n",
    "Values may be encrypted. If key `z` is present the parameters needed for decryption will be provided. These will include the algorithm (generally AES-256), nonce, and a data field which is used to identify the key. The format of the data field will vary depending on the key manager in use, but it will generally provide whatever metadata is required to retreive the key needed for decryption.\n",
    "\n",
    "## Compression\n",
    "\n",
    "Either or both parts may be compressed. If key `c` is present and non-zero, use Zstandard to decompress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742593ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'value 1 header'\n",
      "b'value 1 data'\n",
      "b'value 2 header'\n",
      "b'value 2 data'\n",
      "b'value 3 header'\n",
      "b'value 3 data'\n"
     ]
    }
   ],
   "source": [
    "def decode_value(f: typing.BinaryIO, dlen: int) -> (dict, Optional[bytes]):\n",
    "    \"\"\"\n",
    "    Decode a value from a file-like IO, returning the primary part as a dict\n",
    "    and the secondary part (if present) as raw bytes.\n",
    "    :param f: file-like IO to read from\n",
    "    :param dlen: length of the value\n",
    "    :return: primary value (as dict) and secondary value (bytes or None)\n",
    "    \"\"\"\n",
    "    unpacker = msgpack.Unpacker(f)\n",
    "    val = unpacker.unpack()\n",
    "\n",
    "    if 'z' in val:\n",
    "        # TODO: take apart z to obtain key properties, consult KMS for key, decrypt\n",
    "        raise NotImplementedError('encrypted values not supported')\n",
    "\n",
    "    # Decompress encoded value if compressed\n",
    "    if val.get('c') == 1:\n",
    "        val['e'] = zstd.decompress(val['e'])\n",
    "\n",
    "    primary = msgpack.unpackb(val['e'])\n",
    "    secondary = bytes(0)\n",
    "\n",
    "    try:\n",
    "        sec_enc = val['s'][0]  # Encoding specifier of secondary part\n",
    "        sec_len = sec_enc['l']  # Length of secondary part\n",
    "        f.seek(dlen - sec_len)  # MsgPack may have read the secondary part already, so seek back to it\n",
    "        secondary = f.read(sec_len)\n",
    "\n",
    "        # If secondary encoding specifies compression, or that key is missing and primary encoding specifies\n",
    "        # compression, then decompress the secondary value.\n",
    "        if sec_enc.get('c', val.get('c')) == 1:\n",
    "            secondary = zstd.decompress(secondary)\n",
    "    except IndexError:\n",
    "        pass  # no secondary part\n",
    "    except KeyError:\n",
    "        pass  # no secondary part\n",
    "\n",
    "    return primary, secondary\n",
    "\n",
    "\n",
    "with open('sample_data/3values.tlv', 'rb') as f:\n",
    "    for i in range(3):\n",
    "        header, data = read_tlv(f)\n",
    "        primary, secondary = decode_value(io.BytesIO(data), header.dlen)\n",
    "        pprint(primary)\n",
    "        pprint(secondary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174cd07",
   "metadata": {},
   "source": [
    "# Blocks and Pack Lists\n",
    "\n",
    "The values in LTFS Versioned Object Format may be of one of three types:\n",
    "\n",
    "1. Blocks, which store data.\n",
    "\n",
    "2. Pack list metadata, which describe how blocks are arranged into versions.\n",
    "\n",
    "3. Version metadata, which are described in a later section.\n",
    "\n",
    "This section describes blocks and pack lists. As data enters a LTFS-VOF system, it is split into slices (default 10MB in size) and each slice becomes a block. A version will have one or more blocks. In addition, a pack list is created which shows which ranges of a version are mapped to which ranges of each block.\n",
    "\n",
    "## Blocks\n",
    "\n",
    "Each stored block will have a primary part and secondary part. The primary part will only have key `I` which specifies the composite version ID this block belongs to. The secondary part will contain the raw bytes for the block.\n",
    "\n",
    "### Composite Version IDs\n",
    "\n",
    "A composite Version ID is a combination of the bucket name, object name, and a unique ULID for the version. The format is:\n",
    "\n",
    "    [26 byte version ULID]:bucketname/objectname\n",
    "\n",
    "The `bucketname` will be a bucket name that complies with AWS S3 bucket naming conventions. The `objectname` will be any S3 compliant object name. Note, when parsing, that an object name may contain slashes.\n",
    "\n",
    "The following sample code demonstrates parsing a composite version ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faab0594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VersionID(bucket='bucket', object='object/name.txt', version=ULID(7YGGZJ4YSFMYW6BQVHFKD5KKTV))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VersionID = namedtuple('VersionID', 'bucket object version')\n",
    "\n",
    "def str_to_versionid(version_str: str) -> VersionID:\n",
    "    \"\"\"\n",
    "    Parse a version ID string into a VersionID tuple.\n",
    "    \"\"\"\n",
    "    # First 26 characters is a ULID specifying the version\n",
    "    version = ULID.from_str(version_str[:26])\n",
    "    # Remaining characters (after a ':' separator) are bucket/object name\n",
    "    bucket, object = version_str[27:].split('/', maxsplit=1)\n",
    "    # Together these form the complete version identifier\n",
    "    return VersionID(bucket=bucket, object=object, version=version)\n",
    "\n",
    "\n",
    "def dict_to_versionid(version_dict: dict) -> VersionID:\n",
    "    \"\"\"\n",
    "    Convert dict form of version ID to VersionID tuple.\n",
    "    \"\"\"\n",
    "    return VersionID(\n",
    "        bucket=version_dict['b'],\n",
    "        object=version_dict['o'],\n",
    "        version=ULID.from_str(version_dict['v']),\n",
    "    )\n",
    "\n",
    "str_to_versionid('7YGGZJ4YSFMYW6BQVHFKD5KKTV:bucket/object/name.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c65bb",
   "metadata": {},
   "source": [
    "### Block Parsing\n",
    "\n",
    "The rest of the block parsing is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f9484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Block = namedtuple('Block', 'versionid data')\n",
    "\n",
    "def handle_block(part1: dict, part2: bytes) -> Block:\n",
    "    \"\"\"\n",
    "    Parse a block from decode_value into a Block tuple.\n",
    "    \"\"\"\n",
    "    return Block(versionid=str_to_versionid(part1['I']), data=part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a1531",
   "metadata": {},
   "source": [
    "## Pack Lists\n",
    "\n",
    "In the following figure, consider a version which is split into five blocks, and those blocks are stored across two packs. Version `V1` would require a pack list with two entries, the first referring to a range of `Pack A` which contains its first two blocks, the second referring to `Pack B` which contains the other three blocks.\n",
    "\n",
    "![Blocks and Pack Lists](figures/pack_list.pdf)\n",
    "\n",
    "### Source and Pack Ranges\n",
    "\n",
    "Each entry contains a source range which refers to the version's data without any compression or encryption. It also contains a pack range which refers to where that data is stored, including compression and encryption. When reassembling a version from pack files, the pack range will include the TLV and value header. Thus the decode process should seek to the start of the pack range and expect to decode one or more blocks.\n",
    "\n",
    "### Block and Source Length Lists\n",
    "\n",
    "Each pack list entry also contains two optional lists for block lengths and source lengths.\n",
    "The block lengths list specifies the stored length of each block except the last one; the length of the last block may be inferred from the pack length range minus the other stored block lengths. The source lengths list will usually be empty. If the version was created with a regular stride (e.g. 10MB in this example) then all blocks will have the same source length, except the last which may be shorter. If blocks were created on an irregular stride, then the source lengths list will contain deltas from the version's normal stride. (Again, this is usually zero.)\n",
    "\n",
    "When reassembling whole versions, the block and source lengths lists do not need to be used. They are only required for efficient recall of partial objects.\n",
    "\n",
    "### Pack List Parsing\n",
    "\n",
    "The following code shows how to handle encoded pack lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4968d2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block(versionid=VersionID(bucket='bucket', object='object', version=ULID(7YF1QJW74PNYV552JB3YPAJJX1)), data=b'block 1 data')\n",
      "Block(versionid=VersionID(bucket='bucket', object='object', version=ULID(7YF1QJW74PNYV552JB3YPAJJX1)), data=b'block 2 data')\n",
      "Block(versionid=VersionID(bucket='bucket', object='object', version=ULID(7YF1QJW74PNYV552JB3YPAJJX1)), data=b'block 3 data')\n",
      "PackList(versionid=VersionID(bucket='bucket', object='object', version=ULID(7YF1QJW74PNYV552JB3YPAJJX1)), uploadid=None, packs=[PackEntry(packid=ULID(7YF1QJW74QNR5BZ83NC8307YYM), sourcerange=Range(start=0, len=36), packrange=Range(start=0, len=303), blocklens=[101, 101], sourcelens=[])])\n"
     ]
    }
   ],
   "source": [
    "Range = namedtuple('Range', 'start len')\n",
    "PackEntry = namedtuple('PackEntry', 'packid sourcerange packrange blocklens sourcelens')\n",
    "PackList = namedtuple('PackList', 'versionid uploadid packs')\n",
    "\n",
    "\n",
    "def dict_to_range(range: dict) -> Range:\n",
    "    \"\"\"\n",
    "    Convert dict form of range (from decode_value) to Range tuple.\n",
    "    \"\"\"\n",
    "    return Range(start=range.get('s', 0), len=range.get('l', 0))\n",
    "\n",
    "\n",
    "def dict_to_packentry(pack_entry: dict) -> PackEntry:\n",
    "    \"\"\"\n",
    "    Convert dict form of pack entry (from decode_value) to PackEntry tuple.\n",
    "    \"\"\"\n",
    "    return PackEntry(\n",
    "        packid=ULID.from_str(pack_entry['p']),\n",
    "        sourcerange=dict_to_range(pack_entry['o']),\n",
    "        packrange=dict_to_range(pack_entry['t']),\n",
    "        blocklens=pack_entry.get('E', []),\n",
    "        sourcelens=pack_entry.get('N', []),\n",
    "    )\n",
    "\n",
    "\n",
    "def handle_packlist(part1: dict, part2: Optional[bytes]) -> PackList:\n",
    "    \"\"\"\n",
    "    Parse a pack list from decode_value into a PackList tuple.\n",
    "    \"\"\"\n",
    "    return PackList(versionid=str_to_versionid(part1['I']),\n",
    "                    uploadid=part1.get('U'),\n",
    "                    packs=[dict_to_packentry(p) for p in part1.get('P', [])])\n",
    "\n",
    "\n",
    "\n",
    "def data_pack_reader(f: typing.BinaryIO):\n",
    "    \"\"\"\n",
    "    Scan a data pack file, yielding each entry as a Block or PackList tuple.\n",
    "    :param f: TLV-encoded data pack file\n",
    "    \"\"\"\n",
    "    handlers = {b'bk': handle_block, b'ol': handle_packlist}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            header, data = read_tlv(f)\n",
    "            if header.tag not in handlers:\n",
    "                raise RuntimeError(f'unknown tag {header.tag}; no handler registered')\n",
    "            part1, part2 = decode_value(io.BytesIO(data), header.dlen)\n",
    "            entry = handlers[header.tag](part1, part2)\n",
    "            yield entry\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "\n",
    "with open('sample_data/3blocks.blk', 'rb') as f:\n",
    "    for entry in data_pack_reader(f):\n",
    "        pprint(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dac783-a262-4a6c-a685-1b7e9e12df29",
   "metadata": {},
   "source": [
    "# Versions\n",
    "\n",
    "The last top-level structure in LTFS-VOF is the version record. As mentioned earlier, a version ID is a composite of a ULID, the bucket name, and object name. The version ULIDs sort in order of their creation time with millisecond resolution. Version records will be stored in their own pack files, so the complete history of a bucket can be assembled by reading all the version packs.\n",
    "\n",
    "When reading the version packs, each unique combination of bucket name and object name (that is, each object) will have one or more version records. They should be sorted by version ID to get the correct order of events. The following S3 event types will be represented:\n",
    "\n",
    "1. _Put object_ and _complete multipart upload_ will both create version records with most of their fields filled in. The _delete_ flag will be absent.\n",
    "\n",
    "2. _Delete object_ will create a version with the _delete_ flag set. This is called a delete marker.\n",
    "\n",
    "3. _Delete object_ with a version ID specified will create a special _version delete_ object. This is different from a delete marker, which indicates that the object has been removed. A version delete indicates deletion of a specific version only. The version delete object is required because pack files are append-only and immutable once finalized.\n",
    "\n",
    "The TLV tag for a version record is `vr`. The tag for a version delete record is `vd`.\n",
    "\n",
    "## Version Clones & Data References\n",
    "\n",
    "A copy of a version's data is called a clone. The version records will contain a list of clones under key `p`. The clone structure includes the pool identifier--one of which will be the tape pool--and an encoded data field. The data field encoding may take multiple forms depending on the type of pool. For a tape-based pool, MessagePack is used to encode a structure with the pack list (if small) or a reference to where the pack list is stored (if large). The included code handles both scenarios.\n",
    "\n",
    "## Embedded Version Data\n",
    "\n",
    "If the data of a version is very small (hundreds of bytes) it will be embedded in the version record directly and no data packs or pack lists will be created. The clones list may be empty in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5d626-77c5-449d-8656-d0e31674e195",
   "metadata": {},
   "source": [
    "## Version Decoding Code\n",
    "\n",
    "Code for parsing version records follows. In addition, the generic `ltfsvof_reader` function can be used with any stream of LTFS-VOF encoded data, both data and version packs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "792e5462-78b7-43e1-83d4-e0f2aa160d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACL = namedtuple('ACL', 'idtype id permissions')\n",
    "CryptData = namedtuple('CryptData', 'type datakey extra')\n",
    "Clone = namedtuple('Clone', 'pool data flags blocklen len')\n",
    "Version = namedtuple('Version', 'versionid owner acls len etag deletemarker nullversion '\n",
    "                                'crypt clones metadata usermetadata legalhold data')\n",
    "VersionDelete = namedtuple('VersionDelete', 'versionid deleteid')\n",
    "\n",
    "\n",
    "def dict_to_acl(acl_dict: dict) -> ACL:\n",
    "    \"\"\"\n",
    "    Convert dict form of ACL (from decode_value) to ACL tuple.\n",
    "    \"\"\"\n",
    "    return ACL(\n",
    "        idtype=acl_dict['t'],  # 0: user, 1: group\n",
    "        id=acl_dict['i'],  # user/group ID\n",
    "        permissions=acl_dict['p'],  # 1: read, 2: write, 4: read acl, 8: write acl\n",
    "    )\n",
    "\n",
    "\n",
    "def dict_to_cryptdata(cryptdata_dict: dict) -> Optional[CryptData]:\n",
    "    \"\"\"\n",
    "    Convert dict form of cryptdata (from decode_value) to CryptData tuple.\n",
    "    \"\"\"\n",
    "    if cryptdata_dict is None:\n",
    "        return None\n",
    "\n",
    "    return CryptData(\n",
    "        type=cryptdata_dict['x'],  # 0: none, 1: customer managed key, 2: S3 managed key\n",
    "        datakey=cryptdata_dict['k'],  # encrypted data key or MD5 of customer key\n",
    "        extra=cryptdata_dict['e'],  # extra string data\n",
    "    )\n",
    "\n",
    "\n",
    "def dict_to_clone(clone_dict: dict) -> Clone:\n",
    "    \"\"\"\n",
    "    Convert dict form of clone (from decode_value) to Clone tuple.\n",
    "    \"\"\"\n",
    "    # TODO: take apart MessagePack-encoded data field\n",
    "    return Clone(\n",
    "        pool=clone_dict['p'],\n",
    "        data=clone_dict['l'],\n",
    "        flags=clone_dict['f'],\n",
    "        blocklen=clone_dict['B'],\n",
    "        len=clone_dict['s'],\n",
    "    )\n",
    "\n",
    "\n",
    "def handle_version(part1: dict, part2: Optional[bytes]):\n",
    "    \"\"\"\n",
    "    Parse a version from decode_value into a Version tuple.\n",
    "    \"\"\"\n",
    "    return Version(\n",
    "        versionid=dict_to_versionid(part1),\n",
    "        owner=part1.get('w'),\n",
    "        acls=[dict_to_acl(a) for a in part1.get('A', [])],\n",
    "        len=part1.get('l'),\n",
    "        etag=part1.get('e'),\n",
    "        deletemarker=part1.get('d', False),\n",
    "        nullversion=part1.get('N', False),\n",
    "        crypt=dict_to_cryptdata(part1.get('C')),\n",
    "        clones=[dict_to_clone(c) for c in part1.get('p', [])],\n",
    "        metadata=part1.get('s', {}),\n",
    "        usermetadata=part1.get('m', {}),\n",
    "        legalhold=part1.get('h', False),\n",
    "        data=part1.get('D'))\n",
    "\n",
    "\n",
    "def handle_version_delete(part1: dict, part2: Optional[bytes]):\n",
    "    \"\"\"\n",
    "    Parse a version delete from decode_value into a VersionDelete tuple.\n",
    "    \"\"\"\n",
    "    # TODO: update for version delete structure once tags are known\n",
    "    return VersionDelete(\n",
    "        versionid=dict_to_versionid(part1),\n",
    "        deleteid=part1['???'],\n",
    "    )\n",
    "\n",
    "\n",
    "def ltfsvof_reader(f: typing.BinaryIO):\n",
    "    \"\"\"\n",
    "    Scan any LTFS-VOF file, yielding each entry as tuple of the appropriate type.\n",
    "    :param f: file-like stream with TLV-encoded blocks or versions\n",
    "    \"\"\"\n",
    "    handlers = {b'bk': handle_block,\n",
    "                b'ol': handle_packlist,\n",
    "                b'vr': handle_version,\n",
    "                b'vd': handle_version_delete}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            header, data = read_tlv(f)\n",
    "            if header.tag not in handlers:\n",
    "                raise RuntimeError(f'unknown tag {header.tag}; no handler registered')\n",
    "            part1, part2 = decode_value(io.BytesIO(data), header.dlen)\n",
    "            entry = handlers[header.tag](part1, part2)\n",
    "            yield entry\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a777c",
   "metadata": {},
   "source": [
    "# Appendix: Tests\n",
    "\n",
    "This section is implements basic tests for the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70477a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TlvTests(unittest.TestCase):\n",
    "    def setUp(self) -> None:\n",
    "        self.tlv_data = base64.b64decode(\"iVRMVg0KGgoAAAAAAAAADuM9tfSfjss2AEMhCAAAuxRkYXRhIGRhdGEgZGF0YQ==\")\n",
    "\n",
    "    def test_read_tlv_header(self):\n",
    "        with io.BytesIO(self.tlv_data) as f:\n",
    "            header = read_tlv_header(f)\n",
    "            self.assertEqual(header.magic, b'\\x89TLV\\x0d\\x0a\\x1a\\x0a')\n",
    "            self.assertEqual(header.dlen, 14)\n",
    "            self.assertEqual(header.hashtype, 8)\n",
    "            self.assertEqual(header.version, 0)\n",
    "            self.assertEqual(header.tag, b'C!')\n",
    "\n",
    "    def test_read_tlv(self):\n",
    "        with io.BytesIO(self.tlv_data) as f:\n",
    "            header, data = read_tlv(f)\n",
    "            self.assertEqual(data, b'data data data')\n",
    "\n",
    "    def test_3tlv(self):\n",
    "        with open('sample_data/3simple.tlv', 'rb') as f:\n",
    "            header, data = read_tlv(f)\n",
    "            self.assertEqual(header.tag, b'bk')\n",
    "            self.assertEqual(data, b'data 1')\n",
    "            header, data = read_tlv(f)\n",
    "            self.assertEqual(header.tag, b'bk')\n",
    "            self.assertEqual(data, b'data 2')\n",
    "            header, data = read_tlv(f)\n",
    "            self.assertEqual(header.tag, b'bk')\n",
    "            self.assertEqual(data, b'data 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a2d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueTests(unittest.TestCase):\n",
    "    def test_value_decode(self):\n",
    "        with open('sample_data/3values.tlv', 'rb') as f:\n",
    "            for i in range(3):\n",
    "                header, data = read_tlv(f)\n",
    "                part1, part2 = decode_value(io.BytesIO(data), header.dlen)\n",
    "                self.assertEqual(part1, bytes(f'value {i + 1} header', 'utf-8'))\n",
    "                self.assertEqual(part2, bytes(f'value {i + 1} data', 'utf-8'))\n",
    "\n",
    "    def test_compressed_value_decode(self):\n",
    "        with open('sample_data/compressed_value.tlv', 'rb') as f:\n",
    "            header, data = read_tlv(f)\n",
    "            part1, part2 = decode_value(io.BytesIO(data), header.dlen)\n",
    "            self.assertEqual(part1, b'header header header header header header header header')\n",
    "            self.assertEqual(part2, b'data data data data data data data data data data data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ea6706e-2fd4-4ddb-b3bd-cbf14beaa9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockTests(unittest.TestCase):\n",
    "    def test_read_block(self):\n",
    "        blocks = []\n",
    "        packlist = None\n",
    "\n",
    "        with open('sample_data/3blocks.blk', 'rb') as f:\n",
    "            for entry in data_pack_reader(f):\n",
    "                # pprint(entry)\n",
    "                if isinstance(entry, Block):\n",
    "                    blocks.append(entry)\n",
    "                else:\n",
    "                    packlist = entry\n",
    "\n",
    "        self.assertEqual(len(blocks), 3)\n",
    "        self.assertEqual(blocks[0].data, b'block 1 data')\n",
    "        self.assertEqual(blocks[1].data, b'block 2 data')\n",
    "        self.assertEqual(blocks[2].data, b'block 3 data')\n",
    "        self.assertEqual(1, len(packlist.packs))\n",
    "        self.assertEqual(0, packlist.packs[0].sourcerange.start)\n",
    "        self.assertEqual(3 * len(b'block x data'), packlist.packs[0].sourcerange.len)\n",
    "\n",
    "        # Furthermore, we can read individual blocks based on the packlist\n",
    "        with open('sample_data/3blocks.blk', 'rb') as f:\n",
    "            # Seek past the first two blocks\n",
    "            f.seek(packlist.packs[0].blocklens[0] + packlist.packs[0].blocklens[1])\n",
    "            # Now read the third block\n",
    "            header, data = read_tlv(f)\n",
    "            self.assertEqual(header.tag, b'bk')\n",
    "            part1, part2 = decode_value(io.BytesIO(data), header.dlen)\n",
    "            block3 = handle_block(part1, part2)\n",
    "            self.assertEqual(block3.data, b'block 3 data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2655ad60-011d-49e7-92b2-70b55bfdf0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VersionTests(unittest.TestCase):\n",
    "    def test_read_version(self):\n",
    "        with open('sample_data/minimal_version.ver', 'rb') as f:\n",
    "            for entry in ltfsvof_reader(f):\n",
    "                pprint(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1e0a135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_read_block (__main__.BlockTests.test_read_block) ... ok\n",
      "test_3tlv (__main__.TlvTests.test_3tlv) ... ok\n",
      "test_read_tlv (__main__.TlvTests.test_read_tlv) ... ok\n",
      "test_read_tlv_header (__main__.TlvTests.test_read_tlv_header) ... ok\n",
      "test_compressed_value_decode (__main__.ValueTests.test_compressed_value_decode) ... ok\n",
      "test_value_decode (__main__.ValueTests.test_value_decode) ... ok\n",
      "test_read_version (__main__.VersionTests.test_read_version) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version(versionid=VersionID(bucket='bucket', object='object', version=ULID(7YF1QTCNCDN7FYSQFD2PFH2DCS)), owner=None, acls=[], len=None, etag=None, deletemarker=False, nullversion=False, crypt=None, clones=[], metadata={}, usermetadata={}, legalhold=False, data=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x107039710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
